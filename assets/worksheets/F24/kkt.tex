\documentclass[12pt]{amsart}
%prepared in AMSLaTeX, under LaTeX2e
\addtolength{\oddsidemargin}{-.6in} 
\addtolength{\evensidemargin}{-.65in}
\addtolength{\topmargin}{-.3in}
\addtolength{\textwidth}{1.2in}
\addtolength{\textheight}{0.85in}

\renewcommand{\baselinestretch}{1.075}

\usepackage{verbatim,fancyvrb}

\usepackage{palatino,amssymb}

\usepackage{tikz}
\usetikzlibrary{arrows.meta}

\newtheorem*{thm}{Theorem}
\newtheorem*{defn}{Definition}

\theoremstyle{definition}
\newtheorem*{example}{Example}
\newtheorem*{problem}{Problem}
\newtheorem*{remark}{Remark}

\newcommand{\mtt}{\texttt}
\usepackage{alltt,xspace}
\newcommand{\mfile}[1]
{\medskip\begin{quote}\scriptsize \begin{alltt}\input{#1.m}\end{alltt} \normalsize\end{quote}\medskip}

%\usepackage[final]{graphicx}

\usepackage[pdftex, colorlinks=true, plainpages=false, linkcolor=blue, citecolor=red, urlcolor=blue]{hyperref}

% macros
\newcommand{\bc}{\mathbf{c}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\eps}{\epsilon}
\newcommand{\grad}{\nabla}
\newcommand{\lam}{\lambda}
\newcommand{\lap}{\triangle}

\newcommand{\ip}[2]{\ensuremath{\left<#1,#2\right>}}

%\renewcommand{\det}{\operatorname{det}}
\newcommand{\onull}{\operatorname{null}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}

\newcommand{\prob}[1]{\bigskip\noindent\textbf{#1.}\quad }
\newcommand{\exer}[2]{\prob{Exercise #2 in Lecture #1}}

\newcommand{\Julia}{\textsc{Julia}\xspace}
\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\Octave}{\textsc{Octave}\xspace}
\newcommand{\Python}{\textsc{Python}\xspace}

\DefineVerbatimEnvironment{mVerb}{Verbatim}{numbersep=2mm,
frame=lines,framerule=0.1mm,framesep=2mm,xleftmargin=4mm,fontsize=\footnotesize}

\newcommand{\ema}{\emach}
\newcommand{\emach}{\eps_{\!_{\text{m}}}}

\newcommand{\ppart}[1]{\quad \textbf{(#1)} }
\newcommand{\epart}[1]{\medskip\noindent\textbf{(#1)} \quad}

\allowdisplaybreaks

\begin{document}
\scriptsize \noindent Math 661 Optimization (Bueler) \hfill 25 November 2024
\normalsize

\medskip\bigskip
\Large
\centerline{KKT conditions for general nonlinear optimization}

\bigskip\medskip
\normalsize

\thispagestyle{empty}

These short notes are about the most general problem we have ever considered in this course, namely this nonlinear constrained optimization problem over $x\in\RR^n$:
\begin{alignat*}{2}
    \text{minimize}   &&  &f(x) \\
    \text{subject to} && \qquad g_i(x) &= 0, \quad i=1,\dots,\ell \\
                      &&       h_i(x) &\ge 0, \quad i=1,\dots,m
\end{alignat*}
These notes supplement section 14.5 of the textbook,\footnote{Griva, Nash, and Sofer, \emph{Linear and Nonlinear Optimization}, 2nd ed., SIAM Press 2009.} where the KKT conditions are given but only for equality or inequality constraints separately.

Define $g(x)$ to be the vector formed from $g_1(x),\dots,g_\ell(x)$, and $h(x)$ from $h_1(x),\dots,$ $h_m(x)$.  In these terms we can write
\begin{equation}
\begin{matrix}
    \text{minimize}   & f(x) \\
    \text{subject to} & g(x) = 0 \\
                      & h(x) \ge 0
\end{matrix} \label{prob}
\end{equation}

We will need two definitions from section 14.5.  All functions $f,g_i,h_i$ are assumed to be as differentiable as needed.

\begin{defn}  \emph{(i)} \, For a feasible point $x$, let $\tilde h(x)$ be the vector of length $\tilde m$ formed from the \emph{active} constraints $h_i(x)$ at $x$, i.e.~for which $h_i(x)=0$.

\smallskip
\noindent \emph{(ii)} \, A feasible point $x_*$ is a \emph{regular point} of the constraints if the matrix
    $$\begin{bmatrix}
    \grad g_1(x_*) & \dots & \grad g_\ell(x_*) & \grad {\tilde h}_1(x_*) & \dots & \grad {\tilde h}_{\tilde m}(x_*)
    \end{bmatrix}$$
has linearly independent columns.  This matrix has $n$ rows and $\ell + \tilde m$ columns. \end{defn}

\noindent In definition (ii), note that the gradient of a scalar-valued function is a column vector.

Conceptually, at a regular point each active constraint provides new information; there are no redundancies.  The inactive inequality constraints are ignored in this definition.  The definition of a regular point is called the \emph{linearly independent constraint qualification (LIQC)} in some books.

The Lagrangian for problem \eqref{prob} is
\begin{align*}
\mathcal{L}(x,\lambda,\mu) &= f(x) - \sum_{i=1}^\ell \lambda_i g_i(x) - \sum_{j=1}^m \mu_j h_j(x) \\
  &= f(x) - \lambda^\top g(x) - \mu^\top h(x)
\end{align*}
where $\lambda\in\RR^\ell$ and $\mu\in\RR^m$ are column vectors.  In the second form we regard $g(x)$ and $h(x)$ as column vectors.  Note that the full $h(x)$, not $\tilde h(x)$, is used here.

\clearpage
The following KKT theorem\footnote{\href{https://en.wikipedia.org/wiki/Karush-Kuhn-Tucker_conditions}{\texttt{en.wikipedia.org/wiki/Karush-Kuhn-Tucker\_conditions}}} states the first-order necessary conditions.

\begin{thm}[Karush-Kuhn-Tucker, 1939 \& 1951]  Suppose $x_*$ is a local minimizer of problem \eqref{prob}, and assume it is a regular point of the constraints.  Then there exist vectors $\lambda_*\in\RR^\ell$ and $\mu_*\in\RR^m$ so that
\begin{align*}
\grad_x \mathcal{L}(x_*,\lambda_*,\mu_*) &= 0 &&\text{stationarity} \\
g(x_*) &= 0  &&\text{primal feasibility: equality constraints} \\
h(x_*) &\ge 0  &&\text{primal feasibility: inequality constraints} \\
\mu_* &\ge 0 &&\text{dual feasibility} \\
\mu_*^\top h(x_*) &= 0 &&\text{complementary slackness}
\end{align*}
\end{thm}

\medskip
\noindent The stationarity condition can be expanded:
\begin{align*}
\grad f(x_*) &= \grad g(x_*) \lambda_* + \grad h(x_*) \mu_* \\
  &= (\lambda_*)_1 \grad g_1(x_*) + \dots + (\lambda_*)_\ell \grad g_\ell(x_*) \\
  &\qquad + (\mu_*)_1 \grad h_1(x_*) + \dots + (\mu_*)_m \grad h_m(x_*)
\end{align*}
This says that the gradient of $f$ at the solution $x_*$ can be written as a linear combination of the gradients of the constraints.  Some of the inequality constraints can be inactive, however, so complementary slackness says that the corresponding multipliers $\mu_i$ are zero.  Thus, in fact, the $\grad f(x_*)$ is a linear combination of the gradients of the \emph{active} constraints.  The fact that $x_*$ is a regular point implies that the linear combination is unique, thus that the Lagrange multipliers $\lambda_*,\mu_*$ are unique.

It will be helpful to see one example with both equality and inequality constraints.  The example has two inequality constraints, so that we can see complementary slackness in action.

\begin{example}
    $$\begin{matrix}
    \text{minimize}   & f(x) = \frac{1}{2} \left(x_1^2 + x_2^2\right) \\
    \text{subject to} & x_1 + x_2 = 1 \\
                      & x_1 \ge 1 \\
                      & x_2 \ge -1
\end{matrix}$$
Here $g_1(x) = x_1 + x_2 -1$, $h_1(x) = x_1 - 1$, and $h_2(x) = x_2 + 1$.

The feasible set is a closed line segment between the points $(1,0)$ and $(2,-1)$ in the $x_1,x_2$ plane.  The objective is essentially the square of the distance to the origin.  Thus, in this simple example problem, $x_*=(1,0)$ is the solution.  At this point the first inequality constraint is active while the second is not.

The Lagrangian is
	$$\mathcal{L}(x,\lambda,\mu) = \frac{1}{2} \left(x_1^2 + x_2^2\right) - \lambda(x_1 + x_2 - 1) - \mu_1 (x_1 - 1) - \mu_2(x_2 + 1),$$
with $\lambda \in \RR^1$ and $\mu \in \RR^2$.  The KKT theorem gives 8 scalar facts that must be true at the solution:
\begin{align*}
x_1 - \lambda_1 - \mu_1 &= 0 &&\text{\emph{stationarity}} \\
x_2 - \lambda_1 - \mu_2 &= 0 \\
x_1 + x_2 -1 &= 0  &&\text{\emph{primal feasibility: equality constraint}} \\
x_1 - 1 &\ge 0  &&\text{\emph{primal feasibility: inequality constraints}} \\
x_2 + 1 &\ge 0 \\
\mu_1 &\ge 0 &&\text{\emph{dual feasibility}} \\
\mu_2 &\ge 0 \\
\mu_1 (x_1 - 1) + \mu_2 (x_2 + 1) &= 0 &&\text{\emph{complementary slackness}}
\end{align*}

Since $x=(1,0)$ is the solution, and only the first inequality constraint is active, it follows that $\mu_2=0$.  The complementary slackness condition is $\mu_1 (x_1-1)=0$, so $x_1=1$.  (We will check that $\mu_1>0$ at the solution, so this logic is correct.)  The system of remaining \emph{equations} is now
\begin{align*}
1 - \lambda_1 - \mu_1 &= 0 \\
x_2 - \lambda_1 &= 0 \\
x_2 &= 0
\end{align*}
This is a set of 3 equations for $x_2,\lambda_1,\mu_1$.  It is easy to see $x_2=0$, $\lambda_1=0$, and $\mu_1=1$.  That is,
   $$(x_1,x_2,\lambda_1,\mu_1,\mu_2) = (1,0,0,1,0).$$
It is easy to check that with these values all 8 KKT conditions hold.  In particular, complementary slackness says $1 \cdot 0 + 0 \cdot 1 = 0$, and strict complementarity holds.
\end{example}

\end{document}

