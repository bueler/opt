\documentclass[11pt]{amsart}
%prepared in AMSLaTeX, under LaTeX2e
\addtolength{\oddsidemargin}{-.75in} 
\addtolength{\evensidemargin}{-.75in}
\addtolength{\topmargin}{-.4in}
\addtolength{\textwidth}{1.4in}
\addtolength{\textheight}{1.0in}

\renewcommand{\baselinestretch}{1.075}

\usepackage{verbatim,fancyvrb}

\usepackage{palatino,amssymb}

\usepackage{tikz}
\usetikzlibrary{arrows.meta}

\newtheorem*{thm}{Theorem}
\newtheorem*{defn}{Definition}
\newtheorem*{example}{Example}
\newtheorem*{problem}{Problem}
\newtheorem*{remark}{Remark}

\newcommand{\mtt}{\texttt}
\usepackage{alltt,xspace}
\newcommand{\mfile}[1]
{\medskip\begin{quote}\scriptsize \begin{alltt}\input{#1.m}\end{alltt} \normalsize\end{quote}\medskip}

%\usepackage[final]{graphicx}

\usepackage[pdftex, colorlinks=true, plainpages=false, linkcolor=blue, citecolor=red, urlcolor=blue]{hyperref}

% macros
\newcommand{\bc}{\mathbf{c}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\eps}{\epsilon}
\newcommand{\grad}{\nabla}
\newcommand{\lam}{\lambda}
\newcommand{\lap}{\triangle}

\newcommand{\ip}[2]{\ensuremath{\left<#1,#2\right>}}

%\renewcommand{\det}{\operatorname{det}}
\newcommand{\onull}{\operatorname{null}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}

\newcommand{\prob}[1]{\bigskip\noindent\textbf{#1.}\quad }
\newcommand{\exer}[2]{\prob{Exercise #2 in Lecture #1}}

\newcommand{\Julia}{\textsc{Julia}\xspace}
\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\Octave}{\textsc{Octave}\xspace}
\newcommand{\Python}{\textsc{Python}\xspace}

\DefineVerbatimEnvironment{mVerb}{Verbatim}{numbersep=2mm,
frame=lines,framerule=0.1mm,framesep=2mm,xleftmargin=4mm,fontsize=\footnotesize}

\newcommand{\ema}{\emach}
\newcommand{\emach}{\eps_{\!_{\text{m}}}}

\newcommand{\ppart}[1]{\quad \textbf{(#1)} }
\newcommand{\epart}[1]{\medskip\noindent\textbf{(#1)} \quad}


\begin{document}
\scriptsize \noindent Math 661 Optimization (Bueler) \hfill 25 November 2024
\normalsize

\medskip\bigskip
\Large
\centerline{KKT conditions for nonlinear optimization}

\bigskip\medskip
\normalsize

\thispagestyle{empty}

Let us start from the most general optimization problem considered in this course.  It is a nonlinear constrained optimization problem over $x\in\RR^n$.  We will assume that all functions are as differentiable as needed:
\begin{alignat*}{2}
    \text{minimize}   &&  &f(x) \\
    \text{subject to} && \qquad g_i(x) &= 0, \quad i=1,\dots,\ell \\
                      &&       h_i(x) &\ge 0, \quad i=1,\dots,m
\end{alignat*}

Define $g(x)$ to be the vector formed from $g_1(x),\dots,g_\ell(x)$, and $h(x)$ to be the vector formed from $h_1(x),\dots,h_m(x)$.  In these terms we can write
\begin{equation}
\begin{matrix}
    \text{minimize}   & f(x) \\
    \text{subject to} & g(x) = 0 \\
                      & h(x) \ge 0
\end{matrix} \label{prob}
\end{equation}

We will need a definition which is given in section 14.5 of the textbook.\footnote{Griva, Nash, and Sofer, \emph{Linear and Nonlinear Optimization}, 2nd ed., SIAM Press 2009.}  For a feasible point $x$, let $\tilde h(x)$ be the column vector formed from the \emph{active} constraints $h_i(x)$ at $x$, i.e.~for which $h_i(x)=0$. Note that the inactive inequality constraints are not relevant in this definition.  This definition is called the \emph{linearly independent constraint qualification (LIQC)} in some books.

\begin{defn}  a feasible point $x_*$ is a \emph{regular point} of the constraints if the matrix
    $$\begin{bmatrix}
    \grad g(x_*) & \grad {\tilde h}(x_*)
    \end{bmatrix}$$
has linearly independent columns.
\end{defn}

The Lagrangian for the problem is
\begin{align*}
\mathcal{L}(x,\lambda,\mu) &= f(x) - \sum_{i=1}^\ell \lambda_i g_i(x) - \sum_{j=1}^m \mu_j h_j(x) \\
  &= f(x) - \lambda^\top g(x) - \mu^\top h(x)
\end{align*}
where $\lambda\in\RR^\ell$ and $\mu\in\RR^m$.  Note that $h(x)$, not $\tilde h(x)$, is used here.

The following KKT theorem\footnote{\href{https://en.wikipedia.org/wiki/Karush-Kuhn-Tucker_conditions}{\texttt{en.wikipedia.org/wiki/Karush-Kuhn-Tucker\_conditions}}} states the first-order necessary conditions.

\begin{thm}[Karush-Kuhn-Tucker, 1939]  Suppose $x_*$ is a local minimizer of problem \eqref{prob}, and assume it is a regular point of the constraints.  Then there exist vectors $\lambda_*\in\RR^\ell$ and $\mu_*\in\RR^m$ so that
\begin{align*}
\grad_x \mathcal{L}(x_*,\lambda_*,\mu_*) &= 0 &&\text{stationarity} \\
g(x_*) &= 0  &&\text{primal feasibility: equality constraints} \\
h(x_*) &\ge 0  &&\text{primal feasibility: inequality constraints} \\
\mu_* &\ge 0 &&\text{dual feasibility} \\
\mu_*^\top h(x_*) &= 0 &&\text{complementary slackness}
\end{align*}
\end{thm}

\medskip
\noindent The stationarity condition can be written
    $$\grad f(x_*) = \grad g(x_*) \lambda_* + \grad h(x_*) \mu_*$$
That is, the gradient of $f$ at the solution can be written as a linear combination of the gradients of the constraints.  For the inequality constraints, however, complementary slackness will cause some of the multipliers $\mu_i$ to be zero.  The fact that $x_*$ is a regular point implies that the linear combination is unique, thust that the Lagrange multipliers $\lambda_*,\mu_*$ are unique.


\end{document}

