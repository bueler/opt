\documentclass[11pt]{amsart}
%\pagestyle{empty} 
\setlength{\topmargin}{-0.5in} % usually -0.25in
\addtolength{\textheight}{1.2in} % usually 1.25in
\addtolength{\oddsidemargin}{-0.95in}
\addtolength{\evensidemargin}{-0.95in}
\addtolength{\textwidth}{1.9in} %\setlength{\parindent}{0pt}

\newcommand{\normalspacing}{\renewcommand{\baselinestretch}{1.1}\tiny\normalsize}
\normalspacing

% macros
\usepackage{amssymb,xspace,alltt,verbatim}
\usepackage[final]{graphicx}
\usepackage[pdftex,colorlinks=true]{hyperref}
\usepackage{fancyvrb}
\usepackage{tikz}

\newtheorem*{lem*}{Lemma}

\newcommand{\bs}{\mathbf{s}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bbf}{\mathbf{f}}

\newcommand{\CC}{{\mathbb{C}}}
\newcommand{\RR}{{\mathbb{R}}}
\newcommand{\eps}{\epsilon}
\newcommand{\ZZ}{{\mathbb{Z}}}
\newcommand{\ZZn}{{\mathbb{Z}}_n}
\newcommand{\NN}{{\mathbb{N}}}
\newcommand{\ip}[2]{\mathrm{\left<#1,#2\right>}}

\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\newcommand{\Log}{\operatorname{Log}}

\newcommand{\grad}{\nabla}

\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\Octave}{\textsc{Octave}\xspace}
\newcommand{\pylab}{\textsc{pylab}\xspace}

\newcommand{\prob}[1]{\bigskip\noindent\textbf{#1.} }
\newcommand{\pts}[1]{(\emph{#1 pts})}

\newcommand{\probpts}[2]{\prob{#1} \pts{#2}}
\newcommand{\ppartpts}[2]{\textbf{(#1)} \pts{#2}}
\newcommand{\epartpts}[2]{\medskip\noindent \textbf{(#1)} \pts{#2}}


% from ../wksheets/simplextemplate.tex
\newcommand{\blankset}{\Big\{\hspace{0.7in}\Big\}}
\newcommand{\blankmatrix}{\begin{bmatrix} \hspace{1.0in} \\ \hspace{1.0in} \\ \hspace{1.0in} \\ \hspace{1.0in} \end{bmatrix}}
\newcommand{\blankcolumn}{\begin{bmatrix} \phantom{mm} \\ \phantom{m} \\ \phantom{m} \\ \phantom{m} \end{bmatrix}}

\newcommand{\boxint}{\boxed{\phantom{m\big|}}}

\newcommand{\longblankset}{\Big\{\hspace{1.5in}\Big\}}
\newcommand{\blanktallcolumn}{\begin{bmatrix} \phantom{mm} \\ \phantom{m} \\ \phantom{m} \\ \phantom{m} \\ \phantom{m} \\ \phantom{m} \end{bmatrix}}
\newcommand{\wideblankmatrix}{\begin{bmatrix} \hspace{2.0in} \\ \hspace{2.0in} \\ \hspace{2.0in} \\ \hspace{2.0in} \end{bmatrix}}

\newcommand{\stacksarrow}[2]{\quad {\small \begin{matrix} #1 \\ \to \\ #2 \end{matrix}} \quad}

\newcommand{\lpstep}{%
\noindent\hrulefill
\small
\begin{align*}
\mathcal{B} &= \blankset, & B &= \blankmatrix, & c_B &= \blankcolumn, & \underline{B x_B = b} \implies x_B = \hat b &= \blankcolumn \\
\mathcal{N} &= \blankset, & N &= \blankmatrix, & c_N &= \blankcolumn
\end{align*}
$$\underline{B^\top y = c_B} \implies y = \blankcolumn \qquad \implies \qquad \underline{\hat c_N = c_N - N^\top y} = \blankcolumn$$

\noindent $\boxed{\hat c_N \ge 0 \text{?: stop with optimum}}$ \quad $\hat c_N \stacksarrow{\text{index of }}{\min} t = \boxint$ \, $\to$ \, $\underline{B \hat A_t = A_t} \implies \hat A_t = \begin{bmatrix} \hat a_{1,t} \\ \vdots \\ \hat a_{m,t} \end{bmatrix} = \blankcolumn$

\medskip
\noindent $\boxed{\hat A_t \le 0 \text{?: stop, unbounded}}$ \quad $\Big\{\frac{\hat b_i}{\hat a_{i,t}}\Big\} = \longblankset \quad \stacksarrow{\text{index of }}{\min \text{over } \hat a_{i,t}>0} \quad s = \boxint$}



\begin{document}
\hfill \Large Name:\underline{\phantom{Ed Bueler really really long long long name}}
\medskip

\scriptsize \noindent Math 661 Optimization (Bueler) \hfill Friday, 18 October 2024
\medskip

\Large\centerline{\textbf{Midterm Exam}}

\smallskip
\large
\begin{center}
\textbf{65 minutes.  No book.  No electronics or internet.  $1/2$ sheet of notes allowed.}

(100 \emph{points possible})
\end{center}

\medskip
\thispagestyle{empty}

\probpts{1}{4}  Given a matrix $A\in \RR^{n\times n}$, define what it means for $A$ to be \emph{positive definite}.
\vfill

\prob{2} \ppartpts{a}{4} Define \emph{convex set} (for a subset $S \subset \RR^n$).
\vfill

\epartpts{b}{4} Define \emph{convex function} (for a real-valued function $f$ defined on a convex set $S\subset \RR^n$).
\vfill

\epartpts{c}{4} For a convex set $S\subset \RR^n$, define what it means for $x\in S$ to be an \emph{extreme point}.
\vfill


\clearpage\newpage
\prob{3} \ppartpts{a}{4}  State the \emph{standard form} of a linear programming problem.
\vfill

\epartpts{b}{4}  For a problem in standard form, define \emph{basic solution}.
\vfill


\prob{4}  Let $f(x) = 2 x_3 x_2 + x_3^2 - x_2 - 2 x_1^2$ for $x\in \RR^3$.

\epartpts{a}{6}   Compute the gradient and Hessian of $f$ at $\tilde x = (-1,1,1)^\top \in \RR^3$.
\vspace{3.0in}

\epartpts{b}{5}   Is $p = (-2,1,0)^\top$ a descent direction for $f$ at $\tilde x$ from part \textbf{(a)}?
\vspace{1.5in}


\clearpage\newpage
\prob{5}  Consider the optimization problem
    $$\begin{matrix}
    \text{minimize}\phantom{x} & \phantom{xxxx}f(x) = \exp(x_1^4 + x_2^2) - x_1^4 + \sin(x_1 x_2 x_3)\\
    \text{subject to} & 2 x_1 - 2 x_2 + x_3 = -1 \\
                      & x_1 + 4 x_2 \ge -5 \\
                      & x_2 \ge -1
    \end{matrix}$$

\epartpts{a}{5}  Is $x=(2,0,-5)^\top$ feasible?
\vfill


\epartpts{b}{5}  Considering all of the constraints, both equality and inequality, which are active and which are inactive at the feasible point $\tilde x=(0,-1,-3)^\top$?
\vfill


\probpts{Extra Credit A}{3}  For $x\in\RR^n$, completely solve the standard-form linear programming problem in which there are no equality constraints:
    $$\begin{matrix}
    \text{minimize}   & z = c^\top x\\
    \text{subject to} & x \ge 0
    \end{matrix} \hspace{5.0in}$$
(\emph{Hint.}  Don't do simplex (or other) method.  Please think about it.  Consider all cases for $c$.)
\vspace{2.5in}


\clearpage\newpage
\prob{6} \epartpts{a}{5}  Sketch the feasible set for the following linear programming problem:
    $$\begin{matrix}
    \text{minimize}\phantom{xxx} & \phantom{x}z = 3 x_1 - 8 x_2 \\
    \text{subject to}\phantom{xx} & 2 x_1 + x_2 \le 15 \\
                      & 3 x_1 - x_2 \ge -2 \\
                      & x_1 \ge 0, x_2 \ge 0
    \end{matrix} \hspace{4.0in}$$
\vfill

\epartpts{b}{5}  Convert the problem in part \textbf{(a)} to standard form.
\vfill

\probpts{7}{4}  Given a feasible set $S\in \RR^n$, and a feasible point $\tilde x \in S$, define what it means for $p\in\RR^n$ to be a \emph{feasible direction}.
\vfill


\clearpage\newpage
\prob{8} \epartpts{a}{5}  Sketch the feasible set for the following linear programming problem:
    $$\begin{matrix}
    \text{maximize}\phantom{xxx} & \phantom{x}z = x_1 + 2 x_2 \\
    \text{subject to}\phantom{xx} & 2 x_1 + x_2 \ge 12 \\
                      & - x_1 + 3 x_2 \le 3 \\
                      & x_1 \ge 0, x_2 \ge 0
    \end{matrix} \hspace{4.0in}$$
\vfill

\epartpts{b}{5}  Convert the problem in part \textbf{(a)} to standard form.
\vfill

\epartpts{c}{5}  At the feasible point $\tilde x = (6,0)^\top$ for the original problem, notice that $p=(0,1)^\top$ is a feasible direction.  What is the maximum $\alpha>0$ so that $x=\tilde x + \alpha p$ is feasible?
\vfill

\epartpts{d}{5}  Does this linear programming problem have a solution?  Explain briefly.
\vfill



\clearpage\newpage
\prob{9} \ppartpts{a}{5}  Consider the standard form linear programming problem
    $$\begin{matrix}
    \text{minimize}\phantom{xxx} & \phantom{x}z = -x_1 + x_2 \\
    \text{subject to}\phantom{xx} & -x_1 + x_2 + x_3 \phantom{+x_4} = 3 \\
                      & 2 x_1 + x_2 \phantom{+x_3} + x_4= 4 \\
                      & x \ge 0
    \end{matrix} \hspace{4.0in}$$
Find a basic feasible solution $x$ with $x_1=0$ and $x_2=0$.
\vspace{1.25in}

\epartpts{b}{8}  Let $x$ be the basic feasible solution from part \textbf{(a)}.  Use the template below to complete one iteration of the (revised) simplex method.  \textbf{At the bottom}, fill in the basic and non-basic variables (indices) at the completion of this first iteration.

\bigskip

\lpstep

\hrulefill

\bigskip\bigskip
\normalsize
\noindent  result: \hspace{0.5in} $\displaystyle \mathcal{B} = \left\{\phantom{\big| adlsadf a adkjf}\right\}, \qquad \mathcal{N} = \left\{\phantom{\big| ada da fdsdf lkjf}\right\}$


\clearpage\newpage
\probpts{10}{8}  Prove the following theorem.

\medskip
\noindent \textbf{Theorem.}  \emph{If $x_*$ is a local minimizer of a convex optimization problem then $x_*$ is also a global minimizer.}

\medskip
\noindent \emph{Proof.}
\vfill

\probpts{Extra Credit B}{2}  Suppose $A\in\RR^{m\times n}$ has full row rank.  Suppose that this orthogonal factorization has been done:
	$$A^\top = QR.$$
(Here $Q$ is square with orthonormal columns, and $R$ is upper triangular.)  Explain how to use this factorization to form a null space matrix $Z$ for $A$.
\vspace{1.5in}

\newpage
\bigskip
\small
\begin{center}
\textsc{blank page for scratch work.  clearly-label anything you want to be graded}
\end{center}
\vfill

\end{document}
