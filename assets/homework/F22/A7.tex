\documentclass[12pt]{amsart}
%prepared in AMSLaTeX, under LaTeX2e
\addtolength{\oddsidemargin}{-.6in} 
\addtolength{\evensidemargin}{-.6in}
\addtolength{\topmargin}{-.4in}
\addtolength{\textwidth}{1.2in}
\addtolength{\textheight}{.6in}

\renewcommand{\baselinestretch}{1.05}

\usepackage{verbatim,fancyvrb}
\usepackage{soul}
\usepackage{palatino}

\newtheorem*{thm}{Theorem}
\newtheorem*{defn}{Definition}
\newtheorem*{example}{Example}
\newtheorem*{problem}{Problem}
\newtheorem*{remark}{Remark}

\newcommand{\mtt}{\texttt}
\usepackage{alltt,xspace}
\newcommand{\mfile}[1]
{\medskip\begin{quote}\scriptsize \begin{alltt}\input{#1.m}\end{alltt} \normalsize\end{quote}\medskip}

\usepackage[final]{graphicx}
\newcommand{\mfigure}[1]{\includegraphics[height=2.5in,
width=3.5in]{#1.eps}}
\newcommand{\regfigure}[2]{\includegraphics[height=#2in,
keepaspectratio=true]{#1.eps}}
\newcommand{\widefigure}[3]{\includegraphics[height=#2in,
width=#3in]{#1.eps}}

\usepackage{amssymb}

\usepackage[pdftex, colorlinks=true, plainpages=false, linkcolor=black, citecolor=red, urlcolor=red]{hyperref}

% macros
\newcommand{\br}{\mathbf{r}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\eps}{\epsilon}
\newcommand{\grad}{\nabla}
\newcommand{\lam}{\lambda}
\newcommand{\lap}{\triangle}

\newcommand{\ip}[2]{\ensuremath{\left<#1,#2\right>}}

%\renewcommand{\det}{\operatorname{det}}
\newcommand{\onull}{\operatorname{null}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}

\newcommand{\Julia}{\textsc{Julia}\xspace}
\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\Octave}{\textsc{Octave}\xspace}
\newcommand{\Python}{\textsc{Python}\xspace}

\newcommand{\prob}[1]{\bigskip\noindent\textbf{#1}\quad }

\newcommand{\chapexers}[2]{\prob{Chapter #1, pages #2, Exercises:}}
\newcommand{\exer}[2]{\prob{Exercise #1}}

\newcommand{\pts}[1]{(\emph{#1 pts}) }
\newcommand{\epart}[1]{\medskip\noindent\textbf{(#1)}\quad }
\newcommand{\ppart}[1]{\,\textbf{(#1)}\quad }

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=ellipse,draw,inner sep=2pt] (char) {#1};}}


\begin{document}
\scriptsize \noindent Math 661 Optimization (Bueler) \hfill 31 October, 2022
\normalsize

\medskip\bigskip

\Large\centerline{\textbf{Assignment \#7}}
\large
\bigskip

\centerline{\textbf{Due Wednesday, 9 November 2022, at the start of class}}
\bigskip
\normalsize

\thispagestyle{empty}

\bigskip
Please read sections 11.3, 11.4, 11.5, 12.1, and 12.2 from the textbook.\footnote{Griva, Nash, and Sofer, \emph{Linear and Nonlinear Optimization}, 2nd ed., SIAM Press 2009.}  Please also read the online slides \emph{Steepest descent needs help} at

  \centerline{\href{https://bueler.github.io/opt/assets/slides/sd.pdf}{\texttt{bueler.github.io/opt/assets/slides/sd.pdf}}}

\bigskip
\noindent \textsc{Do the following exercises} from section 11.3, pages xx:

\begin{itemize}
\item Exercise 3.2
\item Exercise 3.3
\item Exercise 3.4
\item Exercise 3.6
\end{itemize}


\prob{Problem P13.}  Let $c\in\RR^n$ and suppose $Q\in \RR^{n\times n}$ is a symmetric positive definite matrix.  Consider this quadratic function on $x\in\RR^n$:
\begin{equation}
    f(x) = \frac{1}{2} x^\top Q x - c^\top x.   \label{quad}
\end{equation}

\medskip
\renewcommand{\labelenumi}{\textbf{\alph{enumi})}}
\begin{enumerate}
\item Show that $\grad f(x) = Q x - c$.
\item Show that $f$ is strictly convex.  (\emph{Hint.  Use facts from section 2.3.})
\item Suppose $p$ is a descent direction at $x$, so that $p^\top \grad f(x)<0$.  Prove that the exact solution of the line search problem $\min_{\alpha>0} f(x+\alpha p)$ is
    $$\alpha = \frac{-p^\top \grad f(x)}{p^\top Q p}.$$
(\emph{Hint.  Define $g(\alpha)=f(x+\alpha p)$, expand it, and compute $g'(\alpha)$.  Explain why is it important that $p$ is a descent direction.})
\end{enumerate}

\medskip
\prob{Problem P14.}  In the slides I show a \Matlab implementation of steepest descent using back-tracking line search, something we will cover carefully in section 11.5.  If we restrict the objective function $f(x)$ to being quadratic then we can use the result in \textbf{P13 c)} to choose the step size.
\begin{enumerate}
\item Implement steepest descent with optimal step size for quadratic functions \eqref{quad}:

\centerline{\texttt{function z = sdquad(x0,Q,c,tol)}}

\noindent As before, stop when $\|\grad f(x_k)\| < $ \texttt{tol}.  (\emph{Hint.  Only small modifications of code from the slides is needed.  Replace evaluations of $f$ and $\grad f$ by formulas for the quadratic case.  Replace back-tracking by the result from }\textbf{P13 c)}.)
\item Use \texttt{sdquad()} to reproduce the result of Example 12.1 on pages 404--405 of the textbook.  Specifically, you should get $k=216$ iterations using \texttt{tol} $=10^{-8}$.
\item Now change $Q$ to
    $$Q = \begin{pmatrix}
    2.3   &  0.19 & -0.89 \\
    0.19  &  1.84 &  0.32 \\
    -0.89 &  0.32 &  1.86
    \end{pmatrix}$$
but keep the same $c$, $x_0$, and \texttt{tol} as in part \textbf{b)}.  What is $x^*$?  How many iterations does \texttt{sdquad()} need?  Why is this problem easier than part (b)?  (\emph{Hint.  What does} \texttt{eig(Q)} \emph{tell you?})
\end{enumerate}

\medskip
\prob{Problem P15.}  (\emph{This problem extends Exercise 3.8 in section 11.3.})

Though section 2.5 fails to make this very clear, the actual definition of \emph{superlinear convergence} is that
    $$\lim_{k\to\infty} \frac{\|e_{k+1}\|}{\|e_k\|} = 0$$
for sequences $\{x_k\}$ which converge to $x_*$, and of course using $e_k = x_k - x_*$.

\begin{enumerate}
\item Let $\{x_k\}$ be a sequence which converges super-linearly to $x_*$.  Show that
    $$\lim_{k\to\infty} \frac{\|x_{k+1} - x_k\|}{\|x_k - x_*\|} = 1.$$
\item Explain why a super-linearly converging iterative algorithm for approximating some unknown number $x_*$, which stops according to the criterion $\|x_{k+1} - x_k\| < $ \texttt{tol}, for some user-supplied tolerance \texttt{tol} $ > 0$, is probably going to work acceptably.  Does the same apply to iterations which converge only linearly? 
\end{enumerate}


\end{document}
