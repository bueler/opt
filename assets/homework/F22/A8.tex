\documentclass[12pt]{amsart}
%prepared in AMSLaTeX, under LaTeX2e
\addtolength{\oddsidemargin}{-.6in} 
\addtolength{\evensidemargin}{-.6in}
\addtolength{\topmargin}{-.4in}
\addtolength{\textwidth}{1.2in}
\addtolength{\textheight}{.6in}

\renewcommand{\baselinestretch}{1.05}

\usepackage{verbatim,fancyvrb}
\usepackage{soul}
\usepackage{palatino}

\newtheorem*{thm}{Theorem}
\newtheorem*{defn}{Definition}
\newtheorem*{example}{Example}
\newtheorem*{problem}{Problem}
\newtheorem*{remark}{Remark}

\newcommand{\mtt}{\texttt}
\usepackage{alltt,xspace}
\newcommand{\mfile}[1]
{\medskip\begin{quote}\scriptsize \begin{alltt}\input{#1.m}\end{alltt} \normalsize\end{quote}\medskip}

\usepackage[final]{graphicx}
\newcommand{\mfigure}[1]{\includegraphics[height=2.5in,
width=3.5in]{#1.eps}}
\newcommand{\regfigure}[2]{\includegraphics[height=#2in,
keepaspectratio=true]{#1.eps}}
\newcommand{\widefigure}[3]{\includegraphics[height=#2in,
width=#3in]{#1.eps}}

\usepackage{amssymb}

\usepackage[pdftex, colorlinks=true, plainpages=false, linkcolor=black, citecolor=red, urlcolor=red]{hyperref}

% macros
\newcommand{\br}{\mathbf{r}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\eps}{\epsilon}
\newcommand{\grad}{\nabla}
\newcommand{\lam}{\lambda}
\newcommand{\lap}{\triangle}

\newcommand{\ip}[2]{\ensuremath{\left<#1,#2\right>}}

%\renewcommand{\det}{\operatorname{det}}
\newcommand{\onull}{\operatorname{null}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}

\newcommand{\Julia}{\textsc{Julia}\xspace}
\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\Octave}{\textsc{Octave}\xspace}
\newcommand{\Python}{\textsc{Python}\xspace}

\newcommand{\prob}[1]{\bigskip\noindent\textbf{#1}\quad }

\newcommand{\chapexers}[2]{\prob{Chapter #1, pages #2, Exercises:}}
\newcommand{\exer}[2]{\prob{Exercise #1}}

\newcommand{\pts}[1]{(\emph{#1 pts}) }
\newcommand{\epart}[1]{\medskip\noindent\textbf{(#1)}\quad }
\newcommand{\ppart}[1]{\,\textbf{(#1)}\quad }

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=ellipse,draw,inner sep=2pt] (char) {#1};}}


\begin{document}
\scriptsize \noindent Math 661 Optimization (Bueler) \hfill 9 November, 2022
\normalsize

\medskip\bigskip

\Large\centerline{\textbf{Assignment \#8}}
\large
\bigskip

\centerline{\textbf{Due Wednesday, 16 November 2022, at the start of class}}
\bigskip
\normalsize

\thispagestyle{empty}

\bigskip
Please read sections 11.4, 11.5, 12.1, 12.2, and 12.3 from the textbook.\footnote{Griva, Nash, and Sofer, \emph{Linear and Nonlinear Optimization}, 2nd ed., SIAM Press 2009.}  Though quasi-Newton methods are intimidating on the first look, please read 12.3 carefully so that you see how they are generalizations of the secant method (\textbf{P17} below).

\bigskip
\noindent \textsc{Do the following exercises} from section 11.4, pages 374--375:

\begin{itemize}
\item Exercise 4.4
\end{itemize}

\bigskip
\noindent \textsc{Do the following exercises} from section 11.5, pages 385--391:

\begin{itemize}
\item Exercise 5.2
\item Exercise 5.5
\end{itemize}


\bigskip
\noindent \textsc{Do the following exercises} from section 12.2, pages 408--411:

\begin{itemize}
\item Exercise 2.3
\end{itemize}


\bigskip
\noindent \textsc{Do the following exercises} from section 12.3, pages 420--421:

\begin{itemize}
\item Exercise 3.7
\end{itemize}


\medskip
\prob{Problem P16.}  Suppose $x_k \in \RR^n$ is any iterate, and suppose that $\grad f(x_k)$ is a nonzero vector.  Compute all $p \in \RR^n$ which solve the problem
    $$\min_{p\ne 0} \frac{p^\top \grad f(x_k)}{\|p\| \|\grad f(x_k)\|}.$$
(\emph{Hint.}  The solution is brief.  Remember $x^\top y$ is the dot product of vectors!  Explain why you have found all minima.)

\medskip
\prob{Problem P17.}  Consider the one-variable problem
    $$\min_{x\in\RR} f(x)$$
where $f:\RR\to\RR$ is twice continuously-differentiable.  Recall that Newton method for the above minimization problem solves $f'(x)=0$ by the formulas $p_k = - f'(x_k)/f''(x_k)$ and $x_{k+1} = x_k + p_k$.

The \emph{secant method} for minimization only differs from the Newton method by replacing the second derivative with a difference quotient approximation based on the last two iterates:
    $$f''(x_k) \approx \frac{f'(x_k) - f'(x_{k-1})}{x_k - x_{k-1}}.$$
Thus the secant method computes the step (search vector) by
    $$p_k = - \frac{(x_k - x_{k-1}) f'(x_k)}{f'(x_k) - f'(x_{k-1})},$$
with $x_{k+1} = x_k + p_k$ as before.

\renewcommand{\labelenumi}{\textbf{\alph{enumi})}}
\renewcommand{\labelenumii}{\roman{enumii})}
\begin{enumerate}
\item Implement the secant method.  Include a stopping criterion $|f'(x_k)| <$\texttt{tol}.
\item Use your code to accurately solve $\min_{x\in\RR} f(x)$, e.g.~with \texttt{tol}$=10^{-8}$, for the following functions and initial iterates:
    \begin{enumerate}
    \item $f(x)=x^3-2\sin x$, \quad $x_0=0$, \quad $x_1=1$
    \item $f(x)=3x^4-4x^3+3x^2-6x$, \quad $x_0=-1$, \quad $x_1=0$
    \end{enumerate}
\item In part ii) above the exact minimum is at $x_*=1$.  Compute the errors $e_k=x_k-x_*$.  Give evidence that the convergence is superlinear.  Using the notation of section 2.5, what is your estimate of the rate (exponent) $r$?
\end{enumerate}


\end{document}
