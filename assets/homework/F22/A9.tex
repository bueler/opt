\documentclass[12pt]{amsart}
%prepared in AMSLaTeX, under LaTeX2e
\addtolength{\oddsidemargin}{-.7in} 
\addtolength{\evensidemargin}{-.7in}
\addtolength{\topmargin}{-.6in}
\addtolength{\textwidth}{1.4in}
\addtolength{\textheight}{1.1in}

\renewcommand{\baselinestretch}{1.05}

\usepackage{verbatim,fancyvrb}
\usepackage{soul}
\usepackage{palatino}

\newtheorem*{thm}{Theorem}
\newtheorem*{defn}{Definition}
\newtheorem*{example}{Example}
\newtheorem*{problem}{Problem}
\newtheorem*{remark}{Remark}

\newcommand{\mtt}{\texttt}
\usepackage{alltt,xspace}
\newcommand{\mfile}[1]
{\medskip\begin{quote}\scriptsize \begin{alltt}\input{#1.m}\end{alltt} \normalsize\end{quote}\medskip}

\usepackage[final]{graphicx}
\newcommand{\mfigure}[1]{\includegraphics[height=2.5in,
width=3.5in]{#1.eps}}
\newcommand{\regfigure}[2]{\includegraphics[height=#2in,
keepaspectratio=true]{#1.eps}}
\newcommand{\widefigure}[3]{\includegraphics[height=#2in,
width=#3in]{#1.eps}}

\usepackage{amssymb}

\usepackage[pdftex, colorlinks=true, plainpages=false, linkcolor=black, citecolor=red, urlcolor=red]{hyperref}

% macros
\newcommand{\br}{\mathbf{r}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\eps}{\epsilon}
\newcommand{\grad}{\nabla}
\newcommand{\lam}{\lambda}
\newcommand{\lap}{\triangle}

\newcommand{\ip}[2]{\ensuremath{\left<#1,#2\right>}}

%\renewcommand{\det}{\operatorname{det}}
\newcommand{\onull}{\operatorname{null}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}

\newcommand{\Julia}{\textsc{Julia}\xspace}
\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\Octave}{\textsc{Octave}\xspace}
\newcommand{\Python}{\textsc{Python}\xspace}

\newcommand{\prob}[1]{\bigskip\noindent\textbf{#1}\quad }

\newcommand{\chapexers}[2]{\prob{Chapter #1, pages #2, Exercises:}}
\newcommand{\exer}[2]{\prob{Exercise #1}}

\newcommand{\pts}[1]{(\emph{#1 pts}) }
\newcommand{\epart}[1]{\medskip\noindent\textbf{(#1)}\quad }
\newcommand{\ppart}[1]{\,\textbf{(#1)}\quad }

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=ellipse,draw,inner sep=2pt] (char) {#1};}}


\begin{document}
\scriptsize \noindent Math 661 Optimization (Bueler) \hfill 19 November, 2022
\normalsize

\medskip\bigskip

\Large\centerline{\textbf{Assignment \#9 \quad (\emph{revised})}}
\large
\bigskip

\centerline{\textbf{Due Wednesday, 30 November 2022, at the start of class}}
\bigskip
\normalsize

\thispagestyle{empty}

\bigskip
Please read sections 12.3, 12.4, 13.5, 14.1, 14.2, and 14.3 from the textbook.\footnote{Griva, Nash, and Sofer, \emph{Linear and Nonlinear Optimization}, 2nd ed., SIAM Press 2009.}

\bigskip
\noindent \textsc{Do the following exercises} from section 12.3, pages 420--421:

\begin{itemize}
\item Exercise 3.1 \quad \begin{minipage}[t]{4.5in} \emph{Please write your own code, or modify} \href{https://bueler.github.io/opt/assets/codes/sr1bt.m}{\texttt{sr1bt.m}} \emph{to use exact line search.} \end{minipage}
\item Exercise 3.3
\item Exercise 3.4 \quad \begin{minipage}[t]{4.5in} \emph{Hint.  What do you know about a rank one matrix?  Thus you can write it as an outer product.} \end{minipage}
\item Exercise 3.8
\end{itemize}


\bigskip
\noindent \textsc{Do the following exercises} from section 13.5, pages 473--474:

\begin{itemize}
\item \st{Exercise 5.5} \quad \begin{minipage}[t]{4.5in} Removed.  \emph{Old hint:  Assume matrix $B_k$ has a known inverse $H_k=B_k^{-1}$.  Let $v=y_k - B_k s_k$.  Then set $u=-v/(v^\top s_k)$ in Sherman-Morrison.} \end{minipage}
\end{itemize}


\bigskip
\noindent \textsc{Do the following exercises} from section 14.2, pages 489--491:

\begin{itemize}
\item Exercise 2.1
\end{itemize}


\medskip
\prob{Problem P18.}  \emph{This problem simplifies/clarifies Exercise 4.4 in section 12.4.}

\medskip
Apply the forward difference formula $f'(x) \approx (f(x+h)-f(x))/h$ to estimate the gradient of the function
    $$f(x) = \exp(10 x_1 + 2 x_2^2),$$
at the point $x=(-1,1)$.  (Obviously, $f:\RR^2 \to \RR$.)  Assuming that $\eps_{\text{mach}} = 2.2204 \times 10^{-16}$ on the computer you used, how accurate is the approximated gradient when you actually compute using the ``best'' value of $h$ and the ``simpler'' value of $h$, from page 425?  (\emph{Use a norm to quantify the error in the gradient.})

\medskip
\prob{Problem P19.}  Suppose a user wants to solve $\min_{x\in\RR^n} f(x)$ for a smooth objective function $f:\RR^n\to \RR$, but they do not provide a gradient function $\grad f(x)$.  We can still use quasi-Newton if we apply finite differences to replace the missing gradient (section 12.4).  Use this idea to write a new code

\medskip
\centerline{\texttt{function [xk, xklist] = sr1btfd(x0,f,tol,maxiters)}}

\medskip
\noindent which replaces the user-provided gradient in \,\href{https://bueler.github.io/opt/assets/codes/sr1bt.m}{\texttt{sr1bt.m}}\, with a finite-difference gradient.  Test it on the Rosenbrock example, as is done in \,\href{https://bueler.github.io/opt/assets/codes/rosencompare.m}{\texttt{rosencompare.m}}, and give iterations.  (\emph{Don't worry about visualizing, or the contours.})  How would it help if the user provided values for the typical scales of the gradient and the Hessian?

\end{document}
